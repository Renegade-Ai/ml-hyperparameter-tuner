# Hyperparameter Tuning Cheat Sheet

This document summarizes the key hyperparameters used in pipeline across different models, what they control, their effects, and common production practices.

---

## ğŸ”¹ TF-IDF Parameters (common to all)

- **`tfidf__max_features`**

  - Limits vocabulary size to top-N terms.
  - Small â†’ faster, less expressive (risk underfitting).
  - Large â†’ richer, slower (risk overfitting).
  - **Prod**: Adjust based on dataset size (500â€“5000 typical).

- **`tfidf__ngram_range`**
  - (1,1) = unigrams; (1,2) = unigrams + bigrams.
  - Adding bigrams captures context (â€œmachine learningâ€ vs â€œmachineâ€ + â€œlearningâ€).
  - **Prod**: (1,2) common; (1,3) increases feature space a lot.

---

## ğŸ”¹ Random Forest

- **`n_estimators`**
  - Number of trees.
  - â†‘ â†’ more stable, slower.
  - **Prod**: 100â€“500 typical.
- **`max_depth`**
  - Depth of trees.
  - â†‘ â†’ complex, risk overfit; â†“ â†’ simpler, more bias.
  - **Prod**: None (big data), else 10â€“30.
- **`min_samples_split`**
  - Min samples to split a node.
  - â†‘ â†’ smoother, less overfit.
  - **Prod**: 2 or 5.
- **`min_samples_leaf`**
  - Min samples per leaf.
  - â†‘ â†’ smoother predictions.
  - **Prod**: 1 default; 2â€“10 if noisy.
- **`max_features`**
  - Features considered per split.
  - `sqrt` (classification default), `log2` for speed.
  - â†‘ â†’ less randomness, â†“ â†’ better generalization.

---

## ğŸ”¹ Gradient Boosting

- **`n_estimators`**
  - Number of boosting stages.
  - â†‘ â†’ stronger model, slower.
  - **Prod**: 100â€“500 typical.
- **`learning_rate`**
  - Step size for updates.
  - â†“ â†’ stable, needs more trees. â†‘ â†’ faster, risk overfit.
  - **Prod**: 0.05â€“0.1 typical.
- **`max_depth`**
  - Depth of base learners.
  - Shallow = better generalization.
  - **Prod**: 3â€“6.
- **`subsample`**
  - Fraction of samples per tree.
  - <1.0 â†’ randomness, less variance.
  - **Prod**: 0.8â€“0.9 common.
- **`min_samples_split`**
  - Same as RF, affects weak learners.
  - â†‘ â†’ smoother splits.

---

## ğŸ”¹ Logistic Regression

- **`penalty`** (l1, l2, elasticnet)
  - l1 = sparsity; l2 = shrinkage; elasticnet = mix.
  - **Prod**: l2 default, l1 for feature selection.
- **`C`**
  - Inverse reg. strength.
  - â†“ â†’ stronger reg, â†‘ â†’ weaker reg.
  - **Prod**: 0.01â€“10.
- **`solver`**
  - Optimizer choice.
  - liblinear = small data; saga = scalable, supports elasticnet.
  - **Prod**: saga for large text datasets.

---

## ğŸ”¹ Support Vector Machine (SVM)

- **`C`**
  - Regularization parameter.
  - â†“ â†’ wider margin, more bias. â†‘ â†’ narrower, overfit risk.
  - **Prod**: 0.01â€“10 tuned.
- **`kernel`**
  - Feature mapping.
  - linear = text data (scalable). rbf/poly = complex, slower.
  - **Prod**: linear for NLP.
- **`gamma`**
  - For rbf/poly only.
  - â†“ â†’ smoother; â†‘ â†’ overfit risk.
  - **Prod**: "scale" default.

---

## ğŸ”¹ Neural Network (MLPClassifier/Regressor)

- **`hidden_layer_sizes`**
  - Shape of hidden layers (e.g., (100,), (50,50)).
  - â†‘ layers/units â†’ more capacity, risk overfit.
  - **Prod**: Start small, grow if needed.
- **`learning_rate_init`**
  - Initial step size.
  - â†“ â†’ stable but slow. â†‘ â†’ fast but unstable.
  - **Prod**: 0.001â€“0.01.
- **`alpha`**
  - L2 reg on weights.
  - â†‘ â†’ stronger reg, less overfit.
  - **Prod**: 0.0001 default; tune if overfitting.
- **`activation`**
  - Non-linearity (relu/tanh).
  - relu = fast, standard; tanh sometimes for small data.
  - **Prod**: relu almost always.

---

## ğŸ”¹ Linear Models (Regression)

- **Linear Regression**
  - `fit_intercept`: Learn intercept? Usually True.
  - `normalize`: Deprecated, use StandardScaler.
- **Ridge/Lasso**
  - `alpha`: Reg strength.
    - Ridge = shrinks weights. Lasso = zeros weights (feature selection).
  - **Prod**: alpha tuned via CV.

---

âœ… **Production Tips**

- Tune only the most impactful parameters (others keep defaults).
- Use log scale for ranges (e.g., C, alpha, learning_rate).
- Start with RandomizedSearch for broad sweep, then GridSearch for fine tuning.
